# 02 â€” Metrics

> Think of metrics like debugging tools for your model.
> You wouldn't ship code without testing it. You wouldn't ship a model without measuring it.
> Here's your toolkit.

---

## The Metrics Family

| Metric | Formula | What It Tells You |
|--------|---------|------------------|
| **MSE** | (1/n) Î£(yáµ¢ - Å·áµ¢)Â² | Average squared error |
| **RMSE** | âˆšMSE | Average error in the same units as y |
| **MAE** | (1/n) Î£\|yáµ¢ - Å·áµ¢\| | Average absolute error |
| **RÂ²** | 1 - (SS_res / SS_tot) | Fraction of variance explained |
| **Adjusted RÂ²** | 1 - [(1-RÂ²)(n-1)/(n-p-1)] | RÂ² penalized for extra features |

---

## MSE â€” Mean Squared Error

```
MSE = (1/n) Î£ (yáµ¢ - Å·áµ¢)Â²
```

**What it is:** Average of the squared residuals.

**Good for:** Optimization (it's what we minimize in least squares).

**Bad for:** Interpretation. If you're predicting house prices in dollars, MSE is in dollarsÂ². What does $50,000Â² even mean? Nothing intuitive.

**When big errors matter more:** MSE is your friend. Squaring means an error of 10 counts 100 times more than an error of 1.

---

## RMSE â€” Root Mean Squared Error

```
RMSE = âˆšMSE
```

**What it is:** Square root of MSE. Now it's back in the same units as your target.

**Think of it as:** "On average, my predictions are off by about this much."

**The go-to metric** for regression problems in practice. When someone says "what's your model's error?" they usually want RMSE.

---

## MAE â€” Mean Absolute Error

```
MAE = (1/n) Î£ |yáµ¢ - Å·áµ¢|
```

**What it is:** Average of the absolute residuals. No squaring.

**Key difference from RMSE:** MAE treats all errors equally. An error of 10 is just 10Ã— worse than an error of 1 (not 100Ã— like MSE).

**Use MAE when:** Outliers exist and you don't want them dominating your metric.

ðŸ’¡ **Interview gold:** "RMSE penalizes large errors more heavily than MAE because of the squaring. If your application can tolerate occasional big misses but wants good average performance, use MAE. If big errors are costly (like in financial models), use RMSE."

---

## RÂ² â€” The One Everyone Asks About

```
RÂ² = 1 - (SS_res / SS_tot)

where:
  SS_res = Î£(yáµ¢ - Å·áµ¢)Â²     (sum of squared residuals â€” your model's errors)
  SS_tot = Î£(yáµ¢ - È³)Â²       (total variance â€” errors if you just predicted the mean)
```

### What it REALLY means

**RÂ² answers: "How much better is my model than just predicting the average?"**

- **RÂ² = 1.0** â†’ Your model perfectly explains all the variance. Every prediction is exactly right.
- **RÂ² = 0.0** â†’ Your model is no better than predicting the mean for everything.
- **RÂ² < 0** â†’ Your model is actively **worse** than predicting the mean. Something is very wrong.

### The mental model

Think of it like a report card:
- **SS_tot** = the total variance in your data (the "problem difficulty")
- **SS_res** = the variance your model didn't explain (the "mistakes left over")
- **RÂ²** = what fraction of the problem you actually solved

---

## Tiny Worked Example

Data: Three points, our model predicts Å·.

| Actual (y) | Predicted (Å·) | Residual | ResidualÂ² |
|-----------|--------------|----------|-----------|
| 2 | 2.5 | -0.5 | 0.25 |
| 4 | 3.5 | 0.5 | 0.25 |
| 6 | 5.5 | 0.5 | 0.25 |

**Mean of y:** (2 + 4 + 6) / 3 = 4

**SS_res** = 0.25 + 0.25 + 0.25 = **0.75**

**SS_tot** = (2-4)Â² + (4-4)Â² + (6-4)Â² = 4 + 0 + 4 = **8**

**MSE** = 0.75 / 3 = **0.25**

**RMSE** = âˆš0.25 = **0.5**

**MAE** = (0.5 + 0.5 + 0.5) / 3 = **0.5**

**RÂ²** = 1 - (0.75 / 8) = 1 - 0.094 = **0.906**

Translation: "Our model explains about 91% of the variance in y. On average, we're off by about 0.5 units."

---

## Why RÂ² Can Lie to You

This is where interviews get spicy.

### Trap 1: RÂ² ALWAYS increases when you add features

Even useless features will increase RÂ² (or at least not decrease it). Why? Because the model has more degrees of freedom to fit the training data.

```
Model A: y ~ xâ‚             â†’ RÂ² = 0.80
Model B: y ~ xâ‚ + x_random  â†’ RÂ² = 0.81  (random noise "helped"!)
```

The model isn't better â€” it's just more flexible. This is why **Adjusted RÂ²** exists.

### Trap 2: High RÂ² doesn't mean your model is good

- RÂ² = 0.99 on training data might just mean overfitting
- RÂ² = 0.99 on a tiny dataset means almost nothing
- RÂ² doesn't tell you if the **assumptions** hold

### Trap 3: RÂ² doesn't tell you about individual predictions

An RÂ² of 0.90 means 90% of variance explained. But some individual predictions could still be wildly off. Always look at residual plots.

âš ï¸ **The interview question:** "Is a higher RÂ² always better?" The answer is **no**. A higher RÂ² on training data might indicate overfitting. What matters is RÂ² on **unseen test data**, and even then, you should check that assumptions hold and residuals look well-behaved.

---

## Adjusted RÂ²

```
Adjusted RÂ² = 1 - [(1 - RÂ²)(n - 1) / (n - p - 1)]

where:
  n = number of data points
  p = number of features
```

**What it does:** Penalizes RÂ² for each feature you add. If a feature doesn't improve the model enough to justify its complexity, Adjusted RÂ² will actually **decrease**.

**Use it for:** Comparing models with different numbers of features.

ðŸ’¡ **This is the sentence interviewers want to hear:**
> "RÂ² always increases with more features, so it's unreliable for model comparison. Adjusted RÂ² penalizes model complexity, making it better for feature selection. But for truly rigorous comparison, I'd use cross-validation."

---

## MSE vs MAE â€” When to Use Which

| Situation | Use | Why |
|-----------|-----|-----|
| Big errors are very costly | MSE / RMSE | Squaring amplifies big mistakes |
| Outliers present, you want robustness | MAE | Doesn't over-penalize outliers |
| You want interpretable units | RMSE or MAE | Both in same units as y |
| Optimization / training | MSE | Smooth, differentiable everywhere |
| You're comparing models | RMSE (standardized) | Most commonly used baseline metric |

---

## Quick Reference for Interviews

**"Explain RÂ² to a non-technical person:"**
> "If RÂ² is 0.85, it means our model captures 85% of the pattern in the data. The other 15% is noise or stuff we haven't accounted for."

**"What's the difference between MSE and MAE?"**
> "MSE squares the errors, so big mistakes get punished disproportionately. MAE treats all errors linearly. Use MSE when big errors are expensive; use MAE when you want robustness to outliers."

**"Can RÂ² be negative?"**
> "Yes â€” it means the model is doing worse than just predicting the mean. This can happen with a bad model or when evaluating on test data with a model that overfit the training set."

---

## Key Takeaways

- **MSE** = average squared error. Great for optimization, bad for interpretation.
- **RMSE** = âˆšMSE. Same units as y. Your default reporting metric.
- **MAE** = average absolute error. Robust to outliers.
- **RÂ²** = fraction of variance explained. But it lies â€” always goes up with more features.
- **Adjusted RÂ²** = RÂ² with a complexity penalty. Better for comparing models.
- **Always check test data**, not just training metrics.
