# 01 ‚Äî Math Core

> The goal here isn't to make you a mathematician.
> It's to make you **comfortable** with the math so you can explain it on a whiteboard without sweating.
> Words first, symbols second. Always.

---

## ≈∑ = XŒ≤ ‚Äî In Words First

Before the formula, the idea:

**"My prediction is a weighted sum of the features."**

For a single data point:
> ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çöx‚Çö

- **Œ≤‚ÇÄ** is the intercept (prediction when all features are zero)
- **Œ≤‚ÇÅ, Œ≤‚ÇÇ, ...** are weights that say "for each unit increase in this feature, the prediction changes by this much"
- **x‚ÇÅ, x‚ÇÇ, ...** are the feature values

For ALL data points at once (matrix form):

```
≈∑ = XŒ≤

where:
  X = [n √ó (p+1)] matrix    (n data points, p features + 1 column of ones for intercept)
  Œ≤ = [(p+1) √ó 1] vector    (the weights we're solving for)
  ≈∑ = [n √ó 1] vector        (all predictions stacked up)
```

üí° **Think of it like this:** X is a big spreadsheet of your data. Œ≤ is a recipe that says "multiply each column by this number and add them up." The result ≈∑ is your predictions.

---

## Residuals ‚Äî "How Wrong We Are"

The residual vector:

```
e = y - ≈∑ = y - XŒ≤
```

- **e** is the vector of all mistakes
- Each element e·µ¢ = y·µ¢ - ≈∑·µ¢ (actual minus predicted)
- Positive e·µ¢ ‚Üí we undershot
- Negative e·µ¢ ‚Üí we overshot

**Our goal:** Find Œ≤ that makes e as small as possible.

---

## Least Squares ‚Äî "Punish Big Mistakes More"

We minimize the **sum of squared residuals** (also called RSS, SSE, or "the loss"):

```
L(Œ≤) = Œ£·µ¢ (y·µ¢ - ≈∑·µ¢)¬≤ = ||y - XŒ≤||¬≤ = (y - XŒ≤)·µÄ(y - XŒ≤)
```

Why not just sum the errors?
- They'd cancel out (positive + negative = 0). Useless.

Why not absolute errors?
- Not differentiable at zero. Harder to optimize.

Squaring is the Goldilocks choice: it's smooth, differentiable, convex, and has a unique minimum.

---

## The Normal Equation

**If you forget everything else, remember this:**

```
Œ≤ = (X·µÄX)‚Åª¬π X·µÄy
```

### What it means in English

1. **X·µÄy** ‚Äî how much each feature "agrees" with the target (dot products between feature columns and y)
2. **X·µÄX** ‚Äî how much the features agree with each other (their correlations and magnitudes)
3. **(X·µÄX)‚Åª¬π** ‚Äî "undo" the feature correlations so each feature gets proper credit
4. **The whole thing** ‚Äî give each feature the right weight after accounting for all the other features

### Where it comes from

Remember from the geometry file: residuals are orthogonal to the column space of X.

```
X·µÄe = 0
X·µÄ(y - XŒ≤) = 0
X·µÄy - X·µÄXŒ≤ = 0
X·µÄXŒ≤ = X·µÄy
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy     ‚Üê Just solve for Œ≤
```

That's it. Four lines. The entire derivation is just writing down the orthogonality condition and solving.

### When it works and when it doesn't

| Works when... | Fails when... |
|---------------|---------------|
| X·µÄX is invertible | Features are perfectly collinear (X·µÄX is singular) |
| Dataset fits in memory | n or p is very large (matrix inversion is O(p¬≥)) |
| Number of features < samples | p > n (more features than data points) |

‚ö†Ô∏è **Do NOT over-derive this in interviews.** Write the Normal Equation. Explain where it comes from (orthogonality). Move on. Nobody wants to watch you expand matrix transposes for 5 minutes.

---

## Gradient Descent ‚Äî Why We Need It at Scale

The Normal Equation is elegant, but it requires **inverting a (p+1) √ó (p+1) matrix**, which is O(p¬≥). When p is large (thousands of features), this is slow.

Enter **Gradient Descent**: an iterative approach that takes baby steps toward the minimum.

### The Idea

```
1. Start with random Œ≤
2. Compute the gradient (which direction is "downhill"?)
3. Take a step in that direction
4. Repeat until you converge
```

### The Update Rule

```
Œ≤ := Œ≤ - Œ± ¬∑ ‚àáL(Œ≤)

where:
  Œ± = learning rate (step size)
  ‚àáL(Œ≤) = -2X·µÄ(y - XŒ≤) = gradient of the loss
```

### What the gradient means

- **‚àáL(Œ≤)** points in the direction of **steepest increase** in the loss
- We move in the **opposite direction** (that's the minus sign) to decrease the loss
- **Œ±** controls how big each step is

### The Learning Rate Tradeoff

```
Too small Œ±:                    Too large Œ±:

L |                             L |
  |  \                            |  \      /\    /\
  |   \                           |   \    /  \  /
  |    \                          |    \  /    \/
  |     \                         |     \/
  |      \____                    |
  |           \_____              | (bouncing! never converges)
  +------------------ step        +------------------ step
   (works but painfully slow)
```

üí° **Interview tip:** If asked "Normal Equation vs Gradient Descent," say:
> "Normal Equation gives the exact answer in one step but costs O(p¬≥). Gradient Descent is iterative but scales better to large p and large n, especially with stochastic variants. In practice, for small-to-medium problems I'd use the closed-form; for large-scale problems, SGD."

---

## Gradient Descent Variants (Brief)

| Variant | Uses | Tradeoff |
|---------|------|----------|
| **Batch GD** | All n data points per step | Stable but slow per iteration |
| **Stochastic GD (SGD)** | 1 random point per step | Noisy but fast, good for large n |
| **Mini-batch GD** | k random points per step | Best of both worlds, most common in practice |

‚ö†Ô∏è **Common interview trap:** "Does gradient descent always find the global minimum for linear regression?" **Yes** ‚Äî the loss function is convex (bowl-shaped), so any local minimum is the global minimum. This is NOT true for neural networks.

---

## Putting It All Together

Here's the complete picture:

```
Start with data: X (features) and y (target)
                    |
                    v
           +-----------------+
           | Choose approach: |
           +-----------------+
            /              \
   Small p, data          Large p or n,
   fits in memory         streaming data
        |                      |
        v                      v
  Normal Equation         Gradient Descent
  Œ≤ = (X·µÄX)‚Åª¬πX·µÄy        Œ≤ := Œ≤ - Œ±¬∑‚àáL(Œ≤)
        |                      |
        v                      v
  Exact answer            Approximate answer
  in one step             (converges to exact)
        |                      |
        +----------+-----------+
                   |
                   v
            ≈∑ = XŒ≤ (predictions)
            e = y - ≈∑ (residuals)
```

---

## Key Takeaways

- **≈∑ = XŒ≤** is just "prediction = weighted sum of features"
- **Residuals** are how wrong we are: e = y - ≈∑
- **Least Squares** minimizes squared residuals ‚Äî penalizes big errors more
- **Normal Equation** Œ≤ = (X·µÄX)‚Åª¬πX·µÄy ‚Äî comes from orthogonality, gives exact answer
- **Gradient Descent** ‚Äî iterative alternative that scales to large problems
- For LR, GD always converges to the global minimum (convex loss)

‚ö†Ô∏è **Remember:** The Normal Equation assumes X·µÄX is invertible. If features are perfectly collinear, it breaks. That's one reason regularization (Ridge) exists ‚Äî it adds ŒªI to X·µÄX, making it always invertible.
