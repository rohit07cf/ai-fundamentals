# 01 â€” Math Core

> The goal here isn't to drown you in symbols.
> It's to make you **comfortable** with the math pipeline so you can whiteboard it without hesitation.
> Words first. Symbols second. Always.

---

## The Full Pipeline (Words First)

Here's everything logistic regression does, in plain English:

```
1. Take the features         â†’  "Here's what I know about this data point"
2. Compute a linear score    â†’  "Weight each feature, add them up"
3. Squeeze through sigmoid   â†’  "Convert that score into a probability"
4. Apply a threshold         â†’  "Make a yes/no decision"
```

That's the entire model. Four steps. Now let's add the math.

---

## Step 1: The Linear Combination â€” z = XÎ²

**In words:** "Multiply each feature by its weight, add them up, include a bias term."

**In math:**

```
z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚šxâ‚š

Or in matrix form: z = XÎ²
```

- **Î²â‚€** â€” the intercept (bias). Shifts the decision boundary left/right.
- **Î²â‚, Î²â‚‚, ...** â€” feature weights. How much each feature contributes to the score.
- **z** â€” the "log-odds" or "logit." Can be any real number.

This is identical to linear regression so far. The magic happens next.

---

## Step 2: The Sigmoid â€” Ïƒ(z) â†’ Probability

**In words:** "Take that score and squeeze it into a probability between 0 and 1."

**In math:**

```
P(y = 1 | X) = Ïƒ(z) = 1 / (1 + e^(-z))
```

This function has the perfect properties for probability:
- **Always between 0 and 1** âœ“
- **Monotonically increasing** â€” higher z = higher probability âœ“
- **Smooth and differentiable** â€” gradient descent loves it âœ“
- **Ïƒ(0) = 0.5** â€” natural midpoint âœ“

ğŸ’¡ **Think of it like this:** z is the model's "raw opinion" (can be any number). The sigmoid is the translator that converts that raw opinion into a calibrated bet.

---

## Step 3: Why the Output Is Between 0 and 1

Let's trace through the math:

```
When z â†’ +âˆ:  e^(-z) â†’ 0    so Ïƒ(z) â†’ 1/(1+0) = 1
When z â†’ -âˆ:  e^(-z) â†’ âˆ    so Ïƒ(z) â†’ 1/(1+âˆ) = 0
When z = 0:   e^0 = 1        so Ïƒ(z) â†’ 1/(1+1) = 0.5
```

The sigmoid asymptotically approaches 0 and 1 but never actually reaches them. It's always a **probability**, never a certainty.

---

## The Log-Odds Connection (Interview Gold)

This is the part most people skip but interviewers love to probe.

The **odds** of class 1 are:

```
odds = P(y=1) / P(y=0) = P / (1 - P)
```

The **log-odds** (or **logit**) are:

```
log(P / (1 - P)) = z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ...
```

**What this means:** Logistic regression assumes the **log-odds** of the positive class are a **linear function** of the features.

This is the fundamental assumption. Not that P is linear in X (it's not â€” it's an S-curve). But that **log(P/(1âˆ’P))** is linear in X.

### Coefficient Interpretation Through Odds

A coefficient of Î²â‚ = 0.7 means:

> "For every 1-unit increase in xâ‚, the **log-odds** increase by 0.7, which means the **odds are multiplied by e^0.7 â‰ˆ 2.01**."

In other words, the odds roughly **double** for each unit increase.

| Coefficient | Effect on Log-Odds | Effect on Odds |
|:-----------:|:-----------------:|:--------------:|
| Î² = 0 | No effect | Odds unchanged (Ã— 1) |
| Î² = 0.5 | +0.5 | Odds Ã— 1.65 |
| Î² = 1.0 | +1.0 | Odds Ã— 2.72 |
| Î² = âˆ’1.0 | âˆ’1.0 | Odds Ã— 0.37 (reduced) |

âš ï¸ **Do NOT confuse this with linear regression:** In linear regression, Î²â‚ = 0.7 means "y increases by 0.7." In logistic regression, Î²â‚ = 0.7 means "log-odds increase by 0.7" â€” the effect on probability is **non-linear** and depends on where you are on the sigmoid curve.

---

## The Full Mathematical Model

Putting it all together:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Features         â”‚  Linear     â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   xâ‚, xâ‚‚, ...  â†’  â”‚  Combinationâ”‚  â†’ z â†’  â”‚ Sigmoid  â”‚  â†’ P(y=1)  â†’ threshold â†’ Å·
                    â”‚  z = XÎ²     â”‚         â”‚ Ïƒ(z)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   This part is               This part converts      This part
   EXACTLY linear              to probability          makes the
   regression                                          decision
```

### Writing it compactly

```
P(y = 1 | X) = Ïƒ(XÎ²) = 1 / (1 + e^(-XÎ²))

Decision: Å· = 1  if  P â‰¥ 0.5  (i.e., z â‰¥ 0)
          Å· = 0  if  P < 0.5  (i.e., z < 0)
```

---

## The Derivative of the Sigmoid (Why It's Beautiful)

You don't need to memorize the derivation, but know the result:

```
dÏƒ/dz = Ïƒ(z) Â· (1 - Ïƒ(z))
```

**Why this matters:**
- It makes gradient computation elegant
- The derivative is highest at z = 0 (steepest part of the S-curve) â€” the model learns fastest near the decision boundary
- The derivative approaches 0 at extreme z values â€” this is the **vanishing gradient** problem in deep learning (a connection interviewers might probe)

```
   Sigmoid:                    Its derivative:
   1.0 |        ___            0.25|
       |      /                    |     .
   0.5 |    /                  0.12|   .   .
       |  /                        | .       .
   0.0 |/                     0.0 |.           .
       +----------â†’ z              +-----------â†’ z

   S-shaped                    Bell-shaped (peaks at z=0)
```

ğŸ’¡ **Aha moment:** The derivative is maximized at the decision boundary (z = 0). This means the model adjusts its weights most aggressively for data points it's least sure about. Intuitively, it focuses its learning effort where it matters most.

---

## Quick Numerical Walk-Through

Model: z = âˆ’2 + 3xâ‚ + (âˆ’1)xâ‚‚

| xâ‚ | xâ‚‚ | z | Ïƒ(z) | Prediction |
|:---:|:---:|:--:|:----:|:----------:|
| 0 | 0 | âˆ’2 | 0.12 | Class 0 |
| 1 | 0 | +1 | 0.73 | Class 1 |
| 1 | 1 | 0 | 0.50 | Boundary! |
| 2 | 1 | +3 | 0.95 | Class 1 (confident) |
| 0 | 3 | âˆ’5 | 0.01 | Class 0 (very confident) |

Decision boundary: âˆ’2 + 3xâ‚ âˆ’ xâ‚‚ = 0 â†’ **xâ‚‚ = 3xâ‚ âˆ’ 2**

---

## What You Need on the Whiteboard

If asked to write the logistic regression model, write exactly this:

```
1.  z = Î²â‚€ + Î²â‚xâ‚ + ... + Î²â‚šxâ‚š         (linear score)
2.  P(y=1|X) = 1 / (1 + e^(-z))          (sigmoid â†’ probability)
3.  log(P/(1-P)) = z                      (log-odds are linear!)
4.  Loss: L = -Î£ [yáµ¢ log(páµ¢) + (1-yáµ¢)log(1-páµ¢)]   (cross-entropy)
5.  Optimize with gradient descent         (no closed-form solution)
```

That's five lines. That's the whole algorithm on a whiteboard. Clean, complete, impressive.

---

## Key Takeaways

- **z = XÎ²** computes a linear score (same as linear regression)
- **Ïƒ(z)** converts the score to a probability between 0 and 1
- The model assumes **log-odds are linear** in the features
- Coefficients affect **odds multiplicatively** (e^Î²), not probabilities additively
- Sigmoid derivative = Ïƒ(z)(1 âˆ’ Ïƒ(z)) â€” peaks at the decision boundary
- **No closed-form solution** â€” we must use iterative optimization

âš ï¸ **The biggest interview mistake:** Saying "logistic regression predicts a line." It doesn't. It predicts an **S-curve** of probabilities. The decision boundary is a line, but the output function is non-linear.
